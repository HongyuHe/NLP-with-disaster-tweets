{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP on tweets - basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the required imports, and load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy as spc\n",
    "from spacy import displacy as dsp\n",
    "train_df = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tailor the NLP pipeline to our purposes\n",
    "\n",
    "Useful for reference:\n",
    "* [rule-based matching](https://spacy.io/usage/rule-based-matching)\n",
    "* [pipelines](https://spacy.io/usage/processing-pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the base pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spc.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to make sure that '@' and '#' get the same treatment. By default, '@' is considered a part of a token, and '#' is considered its own token. So, make sure that they are considered individual tokens, to make processing easier in later parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = nlp.Defaults.prefixes + (r'@',r'#')\n",
    "prefix_regex = spc.util.compile_prefix_regex(prefixes)\n",
    "nlp.tokenizer.prefix_search = prefix_regex.search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this is what the tokenizer does now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@\n",
      "username\n",
      "text\n",
      "#\n",
      "hashtag\n"
     ]
    }
   ],
   "source": [
    "for tok in nlp(\"@username text #hashtag\"):\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the part of the pipeline that combines the '@' and '#' symbols, followed by alphanumerics, into a single token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retokenize_pipe(doc):\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for tok in doc:\n",
    "            if (tok.text == '#' or tok.text == '@') and not bool(tok.whitespace_):\n",
    "                retokenizer.merge(doc[tok.i:tok.i+2])\n",
    "            elif tok.text == '&' and not bool(tok.whitespace_):\n",
    "                retokenizer.merge(doc[tok.i:tok.i+3])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, the tweets contain HTML entities. Some should be replaced with their equivalent words like ('&' to 'and'), others should be removed/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_htmlents_pipe(doc):\n",
    "    replacements = {\n",
    "        '&amp;': 'and',\n",
    "        '&deg;': 'degrees'\n",
    "    }\n",
    "    words = []\n",
    "    has_space = []\n",
    "    for t in doc:\n",
    "        if t.text.startswith('&') and t.text.endswith(';'):\n",
    "            if t.text in replacements:\n",
    "                words.append(replacements[t.text])\n",
    "                has_space.append(t.whitespace_)\n",
    "        else:\n",
    "            words.append(t.text)\n",
    "            has_space.append(t.whitespace_)\n",
    "\n",
    "    \n",
    "    return spc.tokens.Doc(doc.vocab, words=words, spaces=has_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain mentions should be treated as special, i.e. if they mention an account belonging to a news/disaster relief organisation. The presence of such a handle would be a strong indicator of the tweet being about a disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load\n",
    "with open('twitter_handles.json') as f:\n",
    "    twitter_handles = load(f)\n",
    "\n",
    "is_news_mention = lambda t: t.text.startswith('@') and t.text[1:] in twitter_handles['news']\n",
    "is_relief_mention = lambda t: t.text.startswith('@') and t.text[1:] in twitter_handles['relief']\n",
    "spc.tokens.Token.set_extension(\"is_news_mention\", getter=is_news_mention, force=True)\n",
    "spc.tokens.Token.set_extension(\"is_relief_mention\", getter=is_relief_mention, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extensions set above can be called with e.g. `Token._.is_news_mention`, they're not pipeline components. `force=True` is set because Jupyter would complain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the part of the pipeline which takes a '@xxxx' or '#xxx' symbol and marks it as the correct entity. Also marks links as link entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_pipe(nlp):\n",
    "    ruler = nlp.create_pipe(\"entity_ruler\")\n",
    "    \n",
    "    patterns = [\n",
    "        {\"label\": \"HASHTAG\", \"pattern\": [{\"TEXT\": {\"REGEX\": r'^#\\w+'}}]},\n",
    "        {\"label\": \"LINK\", \"pattern\": [{\"TEXT\": {\"REGEX\": r'https?://.*'}}]},\n",
    "        {\"label\": \"NEWS-ORG\", \"pattern\": [{\"_\": {\"is_news_mention\": True}}]},\n",
    "        {\"label\": \"RELIEF-ORG\", \"pattern\": [{\"_\": {\"is_relief_mention\": True}}]},\n",
    "        {\"label\": \"MENTION\", \"pattern\": [{\"TEXT\": {\"REGEX\": r'^@\\w+'}, \"_\": {\"is_news_mention\": False, \"is_relief_mention\": False}}]}\n",
    "    ]\n",
    "    \n",
    "    ruler.add_patterns(patterns)\n",
    "    return ruler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tweets include abbreviations, and these may get incorrectly classified by the default tools. This part of the pipeline should fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbr_handler(nlp):\n",
    "    ruler = nlp.create_pipe(\"entity_ruler\")\n",
    "    patterns = [\n",
    "        {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": {\"REGEX\": r'e\\.?r\\.?'}}]},\n",
    "        {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"emergency\"}, {\"LOWER\": \"room\"}]},\n",
    "        {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"emergency\"}, {\"LOWER\": \"relief\"}]}\n",
    "    ]\n",
    "    ruler.add_patterns(patterns)\n",
    "    return ruler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the functions into the NLP pipeline. The tokenizer is the first thing that runs (implicitly, it's not visible in the pipeline), so the token combiner function should be the first thing in the pipeline. The entity ruler should go before the named entity recogniser, as we want the NER to recognise anything that our custom ruler doesn't recognise, not the other way around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(retokenize_pipe, name=\"retokenizer\", first=True)\n",
    "nlp.add_pipe(handle_htmlents_pipe, name=\"html_ent_handler\", after='retokenizer')\n",
    "nlp.add_pipe(entity_pipe(nlp), name=\"entruler\", before='ner')\n",
    "nlp.add_pipe(abbr_handler(nlp), name=\"abbr_handler\", after='ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current pipeline looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('retokenizer', <function __main__.retokenize_pipe(doc)>),\n",
       " ('html_ent_handler', <function __main__.handle_htmlents_pipe(doc)>),\n",
       " ('tagger', <spacy.pipeline.pipes.Tagger at 0x120f90dd0>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x1217d6b40>),\n",
       " ('entruler', <spacy.pipeline.entityruler.EntityRuler at 0x13fa0b2d0>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x1217d6e50>),\n",
       " ('abbr_handler', <spacy.pipeline.entityruler.EntityRuler at 0x13fa0b3d0>)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the pipeline\n",
    "\n",
    "Now, run the pipeline on a random tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tweet = train_df.sample().iloc[0].text\n",
    "doc = nlp(random_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweet has been tokenized, but not all tokens are useful. In particular, stop words and punctuation are useless for us, so `is_token_allowed` will filter those out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_token_allowed(token):\n",
    "    return (token and token.string.strip() and not token.is_stop and not token.is_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also only want some entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_entity_allowed(entity):\n",
    "    wanted = ['NEWS-ORG', 'RELIEF-ORG', 'HASHTAG', 'ORG', 'GPE', 'FAC']\n",
    "    return entity.label_ in wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, all tokens should be converted to their lowercase, lemmatized form.\n",
    "So, define two hashes containing the results from the processed doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_tokens = [{'token': token.lemma_.strip().lower(), 'pos': token.pos_, 'dep': token.dep_, 'ent': token.ent_type_} for token in doc if is_token_allowed(token)]\n",
    "useful_entities = [{'text': ent.text, 'label': ent.label_} for ent in doc.ents if is_entity_allowed(ent)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract tokens with specific properties to get the general idea of the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sov(toks):\n",
    "    return list({t['token'] for t in toks \n",
    "            if ((t['dep'] in ['nsubj', 'nsubjpass', 'dobj', 'obj', 'pobj']) \n",
    "                or (t['pos'] in ['VERB', 'NOUN', 'PROPN']))\n",
    "            and t['ent'] not in ['LINK', 'MENTION', 'HASHTAG', 'RELIEF-ORG', 'NEWS-ORG']})\n",
    "\n",
    "main_toks = extract_sov(useful_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See the results\n",
    "\n",
    "Finally, print out the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full tweet:\n",
      "\t#hot  Reddit's new content policy goes into effect many horrible subreddits banned or quarantined http://t.co/algtcN8baf #prebreak #best\n",
      "\n",
      "Useful tokens:\n",
      "\t{'token': '#hot', 'pos': 'ADJ', 'dep': 'ROOT', 'ent': 'HASHTAG'}\n",
      "\t{'token': 'reddit', 'pos': 'PROPN', 'dep': 'poss', 'ent': ''}\n",
      "\t{'token': 'new', 'pos': 'ADJ', 'dep': 'amod', 'ent': ''}\n",
      "\t{'token': 'content', 'pos': 'NOUN', 'dep': 'compound', 'ent': ''}\n",
      "\t{'token': 'policy', 'pos': 'NOUN', 'dep': 'nsubj', 'ent': ''}\n",
      "\t{'token': 'go', 'pos': 'VERB', 'dep': 'ROOT', 'ent': ''}\n",
      "\t{'token': 'effect', 'pos': 'NOUN', 'dep': 'pobj', 'ent': ''}\n",
      "\t{'token': 'horrible', 'pos': 'ADJ', 'dep': 'amod', 'ent': ''}\n",
      "\t{'token': 'subreddit', 'pos': 'NOUN', 'dep': 'nsubj', 'ent': ''}\n",
      "\t{'token': 'ban', 'pos': 'VERB', 'dep': 'advcl', 'ent': ''}\n",
      "\t{'token': 'quarantine', 'pos': 'VERB', 'dep': 'conj', 'ent': ''}\n",
      "\t{'token': 'http://t.co/algtcn8baf', 'pos': 'NOUN', 'dep': 'dobj', 'ent': 'LINK'}\n",
      "\t{'token': '#prebreak', 'pos': 'NOUN', 'dep': 'punct', 'ent': 'HASHTAG'}\n",
      "\t{'token': '#b', 'pos': 'ADJ', 'dep': 'nummod', 'ent': 'HASHTAG'}\n",
      "\n",
      "Useful entities:\n",
      "\t{'text': '#hot', 'label': 'HASHTAG'}\n",
      "\t{'text': '#prebreak', 'label': 'HASHTAG'}\n",
      "\t{'text': '#best', 'label': 'HASHTAG'}\n",
      "\n",
      "Gist of the tweet:\n",
      "\t['content', 'reddit', 'subreddit', 'policy', 'ban', 'go', 'quarantine', 'effect']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    #hot\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">HASHTAG</span>\n",
       "</mark>\n",
       "  Reddit's new content policy goes into effect many horrible subreddits banned or quarantined \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #B2FEFA, #0ED2F7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    http://t.co/algtcN8baf\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LINK</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    #prebreak\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">HASHTAG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    #best\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">HASHTAG</span>\n",
       "</mark>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Full tweet:\")\n",
    "print(f'\\t{doc.text}\\n')\n",
    "\n",
    "print(\"Useful tokens:\")\n",
    "for tok in useful_tokens:\n",
    "    print(f'\\t{tok}')\n",
    "\n",
    "print(\"\\nUseful entities:\")\n",
    "\n",
    "for ent in useful_entities:\n",
    "    print(f'\\t{ent}')\n",
    "    \n",
    "print(\"\\nGist of the tweet:\")\n",
    "print(f'\\t{main_toks}')\n",
    "\n",
    "if doc.ents:\n",
    "    dsp.render(doc, style='ent', options={'colors': {'USER': 'linear-gradient(90deg, #fc4a1a, #f7b733)', \n",
    "                                                     'HASHTAG': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)',\n",
    "                                                     'LINK': 'linear-gradient(90deg, #B2FEFA, #0ED2F7)'}})\n",
    "else:\n",
    "    print(\"No entities present.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
