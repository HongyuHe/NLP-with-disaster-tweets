{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP on tweets - basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the required imports, and load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy as spc\n",
    "from spacy import displacy as dsp\n",
    "train_df = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tailor the NLP pipeline to our purposes\n",
    "\n",
    "Useful for reference:\n",
    "* [rule-based matching](https://spacy.io/usage/rule-based-matching)\n",
    "* [pipelines](https://spacy.io/usage/processing-pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the base pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spc.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to make sure that '@' and '#' get the same treatment. By default, '@' is considered a part of a token, and '#' is considered its own token. So, make sure that they are considered individual tokens, to make processing easier in later parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = nlp.Defaults.prefixes + (r'@',r'#')\n",
    "prefix_regex = spc.util.compile_prefix_regex(prefixes)\n",
    "nlp.tokenizer.prefix_search = prefix_regex.search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this is what the tokenizer does now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@\n",
      "username\n",
      "text\n",
      "#\n",
      "hashtag\n"
     ]
    }
   ],
   "source": [
    "for tok in nlp(\"@username text #hashtag\"):\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the part of the pipeline that combines the '@' and '#' symbols, followed by alphanumerics, into a single token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_user_pipe(doc):\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for tok in doc:\n",
    "            if (tok.text == '#' or tok.text == '@') and not bool(tok.whitespace_):\n",
    "                retokenizer.merge(doc[tok.i:tok.i+2])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the part of the pipeline which takes a '@xxxx' or '#xxx' symbol and marks it as a user or hashtag, entity, respectively. Also marks links as link entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_pipe(nlp):\n",
    "    ruler = nlp.create_pipe(\"entity_ruler\")\n",
    "    patterns = [\n",
    "        {\"label\": \"HASHTAG\", \"pattern\": [{\"TEXT\": {\"REGEX\": r'^#\\w+'}}]},\n",
    "        {\"label\": \"USER\", \"pattern\": [{\"TEXT\": {\"REGEX\": r'^@\\w+'}}]},\n",
    "        {\"label\": \"LINK\", \"pattern\": [{\"TEXT\": {\"REGEX\": r'https?://.*'}}]}\n",
    "    ]\n",
    "    ruler.add_patterns(patterns)\n",
    "    return ruler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the two functions into the NLP pipeline. The tokenizer is the first thing that runs (implicitly, it's not visible in the pipeline), so the token combiner function should be the first thing in the pipeline. The entity ruler should go before the named entity recogniser, as we want the NER to recognise anything that our custom ruler doesn't recognise, not the other way around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(hashtag_user_pipe, name=\"retokenizer\", first=True)\n",
    "nlp.add_pipe(entity_pipe(nlp), name=\"entruler\", before='ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current pipeline looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('retokenizer', <function __main__.hashtag_user_pipe(doc)>),\n",
       " ('tagger', <spacy.pipeline.pipes.Tagger at 0x1ada722d0>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x1de3a4d00>),\n",
       " ('entruler', <spacy.pipeline.entityruler.EntityRuler at 0x1dac9b2d0>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x1de3a4ec0>)]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the pipeline\n",
    "\n",
    "Now, run the pipeline on a random tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tweet = train_df.sample().iloc[0].text\n",
    "doc = nlp(random_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweet has been tokenized, but not all tokens are useful. In particular, stop words and punctuation are useless for us, so `is_token_allowed` will filter those out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_token_allowed(token):\n",
    "    return (token and token.string.strip() and not token.is_stop and not token.is_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also only want some entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_entity_allowed(entity):\n",
    "    wanted = ['USER', 'HASHTAG', 'ORG', 'GPE']\n",
    "    return entity.label_ in wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, all tokens should be converted to their lowercase, lemmatized form.\n",
    "So, define two hashes containing the results from the processed doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_tokens = [{'token': token.lemma_.strip().lower(), 'pos': token.pos_, 'dep': token.dep_, 'ent': token.ent_type_} for token in doc if is_token_allowed(token)]\n",
    "useful_entities = [{'text': ent.text, 'label': ent.label_} for ent in doc.ents if is_entity_allowed(ent)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See the results\n",
    "\n",
    "Finally, print out the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'riot', 'pos': 'NOUN', 'dep': 'compound', 'ent': ''}\n",
      "{'token': 'kit', 'pos': 'NOUN', 'dep': 'compound', 'ent': ''}\n",
      "{'token': 'bah', 'pos': 'PROPN', 'dep': 'compound', 'ent': ''}\n",
      "{'token': 'new', 'pos': 'ADJ', 'dep': 'amod', 'ent': ''}\n",
      "{'token': 'concept', 'pos': 'NOUN', 'dep': 'compound', 'ent': ''}\n",
      "{'token': 'gear', 'pos': 'PROPN', 'dep': 'pobj', 'ent': 'ORG'}\n",
      "{'token': 'come', 'pos': 'VERB', 'dep': 'ROOT', 'ent': ''}\n",
      "{'token': 'autumn', 'pos': 'PROPN', 'dep': 'nmod', 'ent': 'DATE'}\n",
      "{'token': 'winter', 'pos': 'PROPN', 'dep': 'pobj', 'ent': 'DATE'}\n",
      "{'token': '#menswear', 'pos': 'NUM', 'dep': 'nummod', 'ent': 'HASHTAG'}\n",
      "{'token': '#fashion', 'pos': 'NUM', 'dep': 'nummod', 'ent': 'HASHTAG'}\n",
      "{'token': '#urbanfashion\\x89รป', 'pos': 'PUNCT', 'dep': 'appos', 'ent': 'HASHTAG'}\n",
      "{'token': 'https://t.co/ccwzdtfbus', 'pos': 'X', 'dep': 'punct', 'ent': 'LINK'}\n",
      "\n",
      "{'text': 'Gear', 'label': 'ORG'}\n",
      "{'text': '#menswear', 'label': 'HASHTAG'}\n",
      "{'text': '#fashion', 'label': 'HASHTAG'}\n",
      "{'text': '#urbanfashion\\x89ร', 'label': 'HASHTAG'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Riot Kit Bah - part of the new concept \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Gear\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " coming for \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Autumn/Winter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "</br>\n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    #menswear\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">HASHTAG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    #fashion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">HASHTAG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    #urbanfashionยร\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">HASHTAG</span>\n",
       "</mark>\n",
       "_ \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #B2FEFA, #0ED2F7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    https://t.co/cCwzDTFbUS\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LINK</span>\n",
       "</mark>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tok in useful_tokens:\n",
    "    print(tok)\n",
    "\n",
    "print()\n",
    "\n",
    "for ent in useful_entities:\n",
    "    print(ent)\n",
    "\n",
    "if doc.ents:\n",
    "    dsp.render(doc, style='ent', options={'colors': {'USER': 'linear-gradient(90deg, #fc4a1a, #f7b733)', \n",
    "                                                     'HASHTAG': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)',\n",
    "                                                     'LINK': 'linear-gradient(90deg, #B2FEFA, #0ED2F7)'}})\n",
    "else:\n",
    "    print(\"No entities present.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
