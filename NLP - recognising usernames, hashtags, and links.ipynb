{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP on tweets - basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the required imports, and load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy as spc\n",
    "from spacy import displacy as dsp\n",
    "train_df = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tailor the NLP pipeline to our purposes\n",
    "\n",
    "Useful for reference:\n",
    "* [rule-based matching](https://spacy.io/usage/rule-based-matching)\n",
    "* [pipelines](https://spacy.io/usage/processing-pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the base pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spc.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to make sure that '@' and '#' get the same treatment. By default, '@' is considered a part of a token, and '#' is considered its own token. So, make sure that they are considered individual tokens, to make processing easier in later parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = nlp.Defaults.prefixes + (r'@',r'#')\n",
    "prefix_regex = spc.util.compile_prefix_regex(prefixes)\n",
    "nlp.tokenizer.prefix_search = prefix_regex.search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this is what the tokenizer does now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@\n",
      "username\n",
      "text\n",
      "#\n",
      "hashtag\n"
     ]
    }
   ],
   "source": [
    "for tok in nlp(\"@username text #hashtag\"):\n",
    "    print(tok)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the part of the pipeline that combines the '@' and '#' symbols, followed by alphanumerics, into a single token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retokenize_pipe(doc):\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for tok in doc:\n",
    "            if not tok.i == len(doc)-1:\n",
    "                if (tok.text == '#' or tok.text == '@') and not (doc[tok.i+1].text == '#' or doc[tok.i+1].text == '@') and not bool(tok.whitespace_):\n",
    "                    retokenizer.merge(doc[tok.i:tok.i+2])\n",
    "                elif tok.text == '&' and not bool(tok.whitespace_) and not tok.i == len(doc)-1 and not '&' in [t.text for t in doc[tok.i+1:tok.i+3]]:\n",
    "                    retokenizer.merge(doc[tok.i:tok.i+3])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, the tweets contain HTML entities. Some should be replaced with their equivalent words like ('&' to 'and'), others should be removed/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_htmlents_pipe(doc):\n",
    "    replacements = {\n",
    "        '&amp;': 'and',\n",
    "        '&deg;': 'degrees'\n",
    "    }\n",
    "    words = []\n",
    "    has_space = []\n",
    "    for t in doc:\n",
    "        if t.text.startswith('&') and t.text.endswith(';'):\n",
    "            if t.text in replacements:\n",
    "                words.append(replacements[t.text])\n",
    "                has_space.append(t.whitespace_)\n",
    "        else:\n",
    "            words.append(t.text)\n",
    "            has_space.append(t.whitespace_)\n",
    "\n",
    "    \n",
    "    return spc.tokens.Doc(doc.vocab, words=words, spaces=has_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain mentions should be treated as special, i.e. if they mention an account belonging to a news/disaster relief organisation. The presence of such a handle would be a strong indicator of the tweet being about a disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load\n",
    "with open('twitter_handles.json') as f:\n",
    "    twitter_handles = load(f)\n",
    "\n",
    "is_news_mention = lambda t: t.text.startswith('@') and t.text[1:] in twitter_handles['news']\n",
    "is_relief_mention = lambda t: t.text.startswith('@') and t.text[1:] in twitter_handles['relief']\n",
    "spc.tokens.Token.set_extension(\"is_news_mention\", getter=is_news_mention, force=True)\n",
    "spc.tokens.Token.set_extension(\"is_relief_mention\", getter=is_relief_mention, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extensions set above can be called with e.g. `Token._.is_news_mention`, they're not pipeline components. `force=True` is set because Jupyter would complain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the part of the pipeline which takes a '@xxxx' or '#xxx' symbol and marks it as the correct entity. Also marks links as link entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_pipe(nlp):\n",
    "    ruler = nlp.create_pipe(\"entity_ruler\")\n",
    "    \n",
    "    patterns = [\n",
    "        {\"label\": \"HASHTAG\", \"pattern\": [{\"TEXT\": {\"REGEX\": r'^#\\w+'}}]},\n",
    "        {\"label\": \"LINK\", \"pattern\": [{\"TEXT\": {\"REGEX\": r'https?://.*'}}]},\n",
    "        {\"label\": \"NEWS-ORG\", \"pattern\": [{\"_\": {\"is_news_mention\": True}}]},\n",
    "        {\"label\": \"RELIEF-ORG\", \"pattern\": [{\"_\": {\"is_relief_mention\": True}}]},\n",
    "        {\"label\": \"MENTION\", \"pattern\": [{\"TEXT\": {\"REGEX\": r'^@\\w+'}, \"_\": {\"is_news_mention\": False, \"is_relief_mention\": False}}]}\n",
    "    ]\n",
    "    \n",
    "    ruler.add_patterns(patterns)\n",
    "    return ruler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tweets include abbreviations, and these may get incorrectly classified by the default tools. This part of the pipeline should fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbr_handler(nlp):\n",
    "    ruler = nlp.create_pipe(\"entity_ruler\")\n",
    "    patterns = [\n",
    "        {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": {\"REGEX\": r'e\\.?r\\.?'}}]},\n",
    "        {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"emergency\"}, {\"LOWER\": \"room\"}]},\n",
    "        {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"emergency\"}, {\"LOWER\": \"relief\"}]}\n",
    "    ]\n",
    "    ruler.add_patterns(patterns)\n",
    "    return ruler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the functions into the NLP pipeline. The tokenizer is the first thing that runs (implicitly, it's not visible in the pipeline), so the token combiner function should be the first thing in the pipeline. The entity ruler should go before the named entity recogniser, as we want the NER to recognise anything that our custom ruler doesn't recognise, not the other way around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(retokenize_pipe, name=\"retokenizer\", first=True)\n",
    "nlp.add_pipe(handle_htmlents_pipe, name=\"html_ent_handler\", after='retokenizer')\n",
    "nlp.add_pipe(entity_pipe(nlp), name=\"entruler\", before='ner')\n",
    "nlp.add_pipe(abbr_handler(nlp), name=\"abbr_handler\", after='ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current pipeline looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('retokenizer', <function __main__.retokenize_pipe(doc)>),\n",
       " ('html_ent_handler', <function __main__.handle_htmlents_pipe(doc)>),\n",
       " ('tagger', <spacy.pipeline.pipes.Tagger at 0x16b5ca410>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x16b5c9a60>),\n",
       " ('entruler', <spacy.pipeline.entityruler.EntityRuler at 0x16d774390>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x16b5c9e50>),\n",
       " ('abbr_handler', <spacy.pipeline.entityruler.EntityRuler at 0x16d774450>)]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the pipeline\n",
    "\n",
    "Now, run the pipeline on a random tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tweet = train_df.sample().iloc[0].text\n",
    "doc = nlp(random_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweet has been tokenized, but not all tokens are useful. In particular, stop words and punctuation are useless for us, so `is_token_allowed` will filter those out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_token_allowed(token):\n",
    "    return (token and token.string.strip() and not token.is_stop and not token.is_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also only want some entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_entity_allowed(entity):\n",
    "    wanted = ['NEWS-ORG', 'RELIEF-ORG', 'HASHTAG', 'ORG', 'GPE', 'FAC']\n",
    "    return entity.label_ in wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, all tokens should be converted to their lowercase, lemmatized form.\n",
    "So, define two hashes containing the results from the processed doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_tokens = [{'token': token.lemma_.strip().lower(), 'pos': token.pos_, 'dep': token.dep_, 'ent': token.ent_type_} for token in doc if is_token_allowed(token)]\n",
    "useful_entities = [{'text': ent.text, 'label': ent.label_} for ent in doc.ents if is_entity_allowed(ent)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract tokens with specific properties to get the general idea of the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sov(toks):\n",
    "    subjects = list({t['token'] for t in toks if t['dep'] in ['nsubj', 'nsubjpass'] and t['ent'] not in ['LINK', 'MENTION', 'HASHTAG', 'RELIEF-ORG', 'NEWS-ORG']})\n",
    "    objects = list({t['token'] for t in toks if t['dep'] in ['dobj, obj, pobj'] and t['ent'] not in ['LINK', 'MENTION', 'HASHTAG', 'RELIEF-ORG', 'NEWS-ORG']})\n",
    "    verbs = list({t['token'] for t in toks if t['pos'] in ['VERB'] and t['ent'] not in ['LINK', 'MENTION', 'HASHTAG', 'RELIEF-ORG', 'NEWS-ORG']})\n",
    "\n",
    "    return subjects, objects, verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See the results\n",
    "\n",
    "Finally, print out the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full tweet:\n",
      "\t@morehouse64 It appears our #Govt has lost an #Ethical and or moral relevance. This means the whole #USA population is in danger from them.\n",
      "\n",
      "Useful tokens:\n",
      "\t{'token': '@morehouse64', 'pos': 'INTJ', 'dep': 'ROOT', 'ent': 'MENTION'}\n",
      "\t{'token': 'appear', 'pos': 'VERB', 'dep': 'ROOT', 'ent': ''}\n",
      "\t{'token': '#govt', 'pos': 'PROPN', 'dep': 'nsubj', 'ent': 'HASHTAG'}\n",
      "\t{'token': 'lose', 'pos': 'VERB', 'dep': 'ccomp', 'ent': ''}\n",
      "\t{'token': '#ethical', 'pos': 'ADJ', 'dep': 'amod', 'ent': 'HASHTAG'}\n",
      "\t{'token': 'moral', 'pos': 'ADJ', 'dep': 'conj', 'ent': ''}\n",
      "\t{'token': 'relevance', 'pos': 'NOUN', 'dep': 'dobj', 'ent': ''}\n",
      "\t{'token': 'mean', 'pos': 'VERB', 'dep': 'ROOT', 'ent': ''}\n",
      "\t{'token': '#usa', 'pos': 'NUM', 'dep': 'compound', 'ent': 'HASHTAG'}\n",
      "\t{'token': 'population', 'pos': 'NOUN', 'dep': 'nsubj', 'ent': ''}\n",
      "\t{'token': 'danger', 'pos': 'NOUN', 'dep': 'pobj', 'ent': 'ORG'}\n",
      "\n",
      "Useful entities:\n",
      "\t{'text': '#Govt', 'label': 'HASHTAG'}\n",
      "\t{'text': '#Ethical', 'label': 'HASHTAG'}\n",
      "\t{'text': '#USA', 'label': 'HASHTAG'}\n",
      "\t{'text': 'danger', 'label': 'ORG'}\n",
      "\n",
      "Gist of the tweet:\n",
      "\t['support', 'teen', 'responder', 'need', 'friend', 'like', 'help']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    @morehouse64\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MENTION</span>\n",
       "</mark>\n",
       " It appears our \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    #Govt\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">HASHTAG</span>\n",
       "</mark>\n",
       " has lost an \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    #Ethical\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">HASHTAG</span>\n",
       "</mark>\n",
       " and or moral relevance. This means the whole \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    #USA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">HASHTAG</span>\n",
       "</mark>\n",
       " population is in \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    danger\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " from them.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Full tweet:\")\n",
    "print(f'\\t{doc.text}\\n')\n",
    "\n",
    "print(\"Useful tokens:\")\n",
    "for tok in useful_tokens:\n",
    "    print(f'\\t{tok}')\n",
    "\n",
    "print(\"\\nUseful entities:\")\n",
    "\n",
    "for ent in useful_entities:\n",
    "    print(f'\\t{ent}')\n",
    "    \n",
    "print(\"\\nGist of the tweet:\")\n",
    "print(f'\\t{main_toks}')\n",
    "\n",
    "if doc.ents:\n",
    "    dsp.render(doc, style='ent', options={'colors': {'USER': 'linear-gradient(90deg, #fc4a1a, #f7b733)', \n",
    "                                                     'HASHTAG': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)',\n",
    "                                                     'LINK': 'linear-gradient(90deg, #B2FEFA, #0ED2F7)'}})\n",
    "else:\n",
    "    print(\"No entities present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id, keyword, location, text, hashtag, subj, verb, object, news org (bool), relief org (bool), org, gpe, fac\n",
    "df_out = pd.DataFrame(columns=['id', 'keyword', 'location', 'text', 'hashtag', 'subj', 'verb', 'obj', 'mentioned_news_org', 'mentioned_relief_org', 'orgs', 'gpes', 'facs'])\n",
    "for i, twit in train_df.iterrows():\n",
    "    doc = nlp(twit.text)\n",
    "    useful_tokens = [{'token': token.lemma_.strip().lower(), 'pos': token.pos_, 'dep': token.dep_, 'ent': token.ent_type_} for token in doc if is_token_allowed(token)]\n",
    "    useful_entities = [{'text': ent.text, 'label': ent.label_} for ent in doc.ents if is_entity_allowed(ent)]\n",
    "    \n",
    "    hashtags = [ent['text'] for ent in useful_entities if ent['label'] == 'HASHTAG']\n",
    "    mentioned_news = bool([ent for ent in useful_entities if ent['label'] == 'NEWS-ORG'])\n",
    "    mentioned_relief = bool([ent for ent in useful_entities if ent['label'] == 'RELIEF-ORG'])\n",
    "    orgs = [ent['text'] for ent in useful_entities if ent['label'] == 'ORG']\n",
    "    gpes = [ent['text'] for ent in useful_entities if ent['label'] == 'GPE']\n",
    "    facs = [ent['text'] for ent in useful_entities if ent['label'] == 'FAC']\n",
    "    subjs, objs, verbs = extract_sov(useful_tokens)\n",
    "    \n",
    "    df_out.loc[i] = {\n",
    "        'id': twit.id,\n",
    "        'keyword': twit.keyword,\n",
    "        'location': twit.location,\n",
    "        'text': twit.text,\n",
    "        'hashtag': hashtags,\n",
    "        'subj': subjs,\n",
    "        'verb': verbs,\n",
    "        'obj': objs,\n",
    "        'mentioned_news_org': mentioned_news,\n",
    "        'mentioned_relief_org': mentioned_relief,\n",
    "        'orgs': orgs,\n",
    "        'gpes': gpes,\n",
    "        'facs': facs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_csv(\"train_data_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text', 'hashtag', 'subj', 'verb', 'obj',\n",
       "       'mentioned_news_org', 'mentioned_relief_org', 'orgs', 'gpes', 'facs'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
