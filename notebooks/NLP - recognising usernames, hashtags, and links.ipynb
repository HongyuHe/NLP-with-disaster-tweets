{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with null:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of rows with null:\")\n",
    "df.apply(lambda x: sum(x.isnull()), axis='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None have missing target (label) or text, which is good. We can use those without issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Target 1 without keyword:  42\n",
      "Num of Target 0 without keyword:  19\n"
     ]
    }
   ],
   "source": [
    "print('Num of Target 1 without keyword: ', df.query('target == 1 and keyword.isnull()').shape[0])\n",
    "print('Num of Target 0 without keyword: ', df.query('target == 0 and keyword.isnull()').shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether or not the tweet is a disaster doesn't _really_ affect whether or not it has a keyword, and there won't be that many to fill in. So let's use the mode of each class to fill them in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_0 = df[df.target == 0].keyword.mode().iloc[0]\n",
    "mode_1 = df[df.target == 1].keyword.mode().iloc[0]\n",
    "\n",
    "df['keyword'] = df.apply(\n",
    "    lambda row: row.keyword if not pd.isna(row.keyword) else (mode_0 if row.target == 0 else mode_1),\n",
    "    axis = 'columns' # apply to each row\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Target 1 without location:  1075\n",
      "Num of Target 0 without location:  1458\n"
     ]
    }
   ],
   "source": [
    "print('Num of Target 1 without location: ', df.query('target == 1 and location.isnull()').shape[0])\n",
    "print('Num of Target 0 without location: ', df.query('target == 0 and location.isnull()').shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few missing for both targets, and the difference is large. Unfortunately we cannot do much with this, as predicting the location from other features (well, mainly the text) would be tough. So just replace nan values with an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['location'] = df.apply(\n",
    "    lambda row: row.location if not pd.isna(row.location) else '',\n",
    "    axis = 'columns' # apply to each row\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict duplicates:  0\n",
      "Description duplicates:  69\n",
      "Number of duplicates with conflicting labels:  37\n"
     ]
    }
   ],
   "source": [
    "print('Strict duplicates: ', df[df.duplicated()].size)\n",
    "duplicated_text_rows = df[df.duplicated(['text'])]\n",
    "print('Description duplicates: ', duplicated_text_rows.text.unique().size)\n",
    "print('Number of duplicates with conflicting labels: ', duplicated_text_rows.apply(lambda onerow: df[onerow.text == df.text].target.unique().size == 1, axis='columns').value_counts()[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dropped duplicate rows:  110\n",
      "As a percentage of total data: 1.44%\n"
     ]
    }
   ],
   "source": [
    "n_removed = df.shape[0]-df.drop_duplicates(['text']).shape[0]\n",
    "print('Number of dropped duplicate rows: ', n_removed)\n",
    "print('As a percentage of total data: %.2f%%' % (float(n_removed)/df.shape[0]*100))\n",
    "df = df.drop_duplicates(['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 3198 (42.62%)\n",
      "Negative: 4305 (57.38%)\n"
     ]
    }
   ],
   "source": [
    "n_positive = df[df.target == 1].shape[0]\n",
    "n_negative = df[df.target == 0].shape[0]\n",
    "n_total = df.shape[0]\n",
    "print(f'Positive: {n_positive} ({round(float(n_positive)/n_total*100, 2)}%)')\n",
    "print(f'Negative: {n_negative} ({round(float(n_negative)/n_total*100, 2)}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, we've determined this is not too bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the required imports, and load the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tailor the NLP pipeline to our purposes\n",
    "\n",
    "Useful for reference:\n",
    "* [rule-based matching](https://spacy.io/usage/rule-based-matching)\n",
    "* [pipelines](https://spacy.io/usage/processing-pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the base pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy as spc\n",
    "from spacy import displacy as dsp\n",
    "nlp = spc.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to make sure that '@' and '#' get the same treatment. By default, '@' is considered a part of a token, and '#' is considered its own token. So, make sure that they are considered individual tokens, to make processing easier in later parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = nlp.Defaults.prefixes + (r'@',r'#')\n",
    "prefix_regex = spc.util.compile_prefix_regex(prefixes)\n",
    "nlp.tokenizer.prefix_search = prefix_regex.search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this is what the tokenizer does now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@\n",
      "username\n",
      "text\n",
      "#\n",
      "hashtag\n"
     ]
    }
   ],
   "source": [
    "for tok in nlp(\"@username text #hashtag\"):\n",
    "    print(tok)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the part of the pipeline that combines the '@' and '#' symbols, followed by alphanumerics, into a single token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retokenize_pipe(doc):\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for tok in doc:\n",
    "            if not tok.i == len(doc)-1:\n",
    "                if (tok.text == '#' or tok.text == '@') and not (doc[tok.i+1].text == '#' or doc[tok.i+1].text == '@') and not bool(tok.whitespace_):\n",
    "                    retokenizer.merge(doc[tok.i:tok.i+2])\n",
    "                elif tok.text == '&' and not bool(tok.whitespace_) and not tok.i == len(doc)-1 and not '&' in [t.text for t in doc[tok.i+1:tok.i+3]]:\n",
    "                    retokenizer.merge(doc[tok.i:tok.i+3])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, the tweets contain HTML entities. Some should be replaced with their equivalent words like ('&' to 'and'), others should be removed/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_htmlents_pipe(doc):\n",
    "    replacements = {\n",
    "        '&amp;': 'and',\n",
    "        '&deg;': 'degrees',\n",
    "        '%20': ' '\n",
    "    }\n",
    "    words = []\n",
    "    has_space = []\n",
    "    for t in doc:\n",
    "        if t.text.startswith('&') and t.text.endswith(';'):\n",
    "            if t.text in replacements:\n",
    "                words.append(replacements[t.text])\n",
    "                has_space.append(t.whitespace_)\n",
    "        else:\n",
    "            words.append(t.text)\n",
    "            has_space.append(t.whitespace_)\n",
    "\n",
    "    \n",
    "    return spc.tokens.Doc(doc.vocab, words=words, spaces=has_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain mentions should be treated as special, i.e. if they mention an account belonging to a news/disaster relief organisation. The presence of such a handle would be a strong indicator of the tweet being about a disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load\n",
    "with open('twitter_handles.json') as f:\n",
    "    twitter_handles = load(f)\n",
    "\n",
    "is_news_mention = lambda t: t.text.startswith('@') and t.text[1:].lower() in twitter_handles['news']\n",
    "is_relief_mention = lambda t: t.text.startswith('@') and t.text[1:].lower() in twitter_handles['relief']\n",
    "spc.tokens.Token.set_extension(\"is_news_mention\", getter=is_news_mention, force=True)\n",
    "spc.tokens.Token.set_extension(\"is_relief_mention\", getter=is_relief_mention, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extensions set above can be called with e.g. `Token._.is_news_mention`, they're not pipeline components. `force=True` is set because Jupyter would complain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the part of the pipeline which takes a '@xxxx' or '#xxx' symbol and marks it as the correct entity. Also marks links as link entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_pipe(nlp):\n",
    "    ruler = nlp.create_pipe(\"entity_ruler\")\n",
    "    \n",
    "    patterns = [\n",
    "        {\"label\": \"HASHTAG\", \"pattern\": [{\"TEXT\": {\"REGEX\": r'^#\\w+'}}]},\n",
    "        {\"label\": \"LINK\", \"pattern\": [{\"TEXT\": {\"REGEX\": r'https?://.*'}}]},\n",
    "        {\"label\": \"NEWS-ORG\", \"pattern\": [{\"_\": {\"is_news_mention\": True}}]},\n",
    "        {\"label\": \"RELIEF-ORG\", \"pattern\": [{\"_\": {\"is_relief_mention\": True}}]},\n",
    "        {\"label\": \"MENTION\", \"pattern\": [{\"TEXT\": {\"REGEX\": r'^@\\w+'}, \"_\": {\"is_news_mention\": False, \"is_relief_mention\": False}}]}\n",
    "    ]\n",
    "    \n",
    "    ruler.add_patterns(patterns)\n",
    "    return ruler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tweets include abbreviations, and these may get incorrectly classified by the default tools. This part of the pipeline should fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbr_handler(nlp):\n",
    "    ruler = nlp.create_pipe(\"entity_ruler\")\n",
    "    patterns = [\n",
    "        {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": {\"REGEX\": r'e\\.?r\\.?'}}]},\n",
    "        {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"emergency\"}, {\"LOWER\": \"room\"}]},\n",
    "        {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"emergency\"}, {\"LOWER\": \"relief\"}]}\n",
    "    ]\n",
    "    ruler.add_patterns(patterns)\n",
    "    return ruler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the functions into the NLP pipeline. The tokenizer is the first thing that runs (implicitly, it's not visible in the pipeline), so the token combiner function should be the first thing in the pipeline. The entity ruler should go before the named entity recogniser, as we want the NER to recognise anything that our custom ruler doesn't recognise, not the other way around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(retokenize_pipe, name=\"retokenizer\", first=True)\n",
    "nlp.add_pipe(handle_htmlents_pipe, name=\"html_ent_handler\", after='retokenizer')\n",
    "nlp.add_pipe(entity_pipe(nlp), name=\"entruler\", before='ner')\n",
    "nlp.add_pipe(abbr_handler(nlp), name=\"abbr_handler\", after='ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current pipeline looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('retokenizer', <function __main__.retokenize_pipe(doc)>),\n",
       " ('html_ent_handler', <function __main__.handle_htmlents_pipe(doc)>),\n",
       " ('tagger', <spacy.pipeline.pipes.Tagger at 0x11fce4890>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x11f959050>),\n",
       " ('entruler', <spacy.pipeline.entityruler.EntityRuler at 0x126db0bd0>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x11fb8fbb0>),\n",
       " ('abbr_handler', <spacy.pipeline.entityruler.EntityRuler at 0x126db0cd0>)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the pipeline\n",
    "\n",
    "Now, run the pipeline on a random tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tweet = df.sample().iloc[0].text\n",
    "doc = nlp(random_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweet has been tokenized, but not all tokens are useful. In particular, stop words and punctuation are useless for us, so `is_token_allowed` will filter those out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_token_allowed(token):\n",
    "    return (token and token.string.strip() and not token.is_stop and not token.is_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also only want some entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_entity_allowed(entity):\n",
    "    wanted = ['NEWS-ORG', 'RELIEF-ORG', 'HASHTAG', 'ORG', 'GPE', 'FAC', 'MENTION']\n",
    "    return entity.label_ in wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, all tokens should be converted to their lowercase, lemmatized form.\n",
    "So, define two functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_useful_tokens(doc):\n",
    "    return [{'token': token.lemma_.strip().lower(), 'pos': token.pos_, 'dep': token.dep_, 'ent': token.ent_type_} \n",
    "             for token in doc \n",
    "             if is_token_allowed(token)]\n",
    "\n",
    "def get_useful_ents(doc):\n",
    "    return [{'text': ent.text, 'label': ent.label_} \n",
    "            for ent in doc.ents \n",
    "            if is_entity_allowed(ent)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract tokens with specific properties to get the general idea of the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sov(toks):\n",
    "    subjects = list({t['token'] for t in toks if t['dep'] in ['nsubj', 'nsubjpass'] and t['ent'] not in ['LINK', 'MENTION', 'HASHTAG', 'RELIEF-ORG', 'NEWS-ORG']})\n",
    "    objects = list({t['token'] for t in toks if t['dep'] in ['dobj', 'obj', 'pobj'] and t['ent'] not in ['LINK', 'MENTION', 'HASHTAG', 'RELIEF-ORG', 'NEWS-ORG']})\n",
    "    verbs = list({t['token'] for t in toks if t['pos'] in ['VERB'] and t['ent'] not in ['LINK', 'MENTION', 'HASHTAG', 'RELIEF-ORG', 'NEWS-ORG']})\n",
    "\n",
    "    return subjects, objects, verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to filter the list of entities for a specific label, so define a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_ents_by_label(label, ent_list, text_only=False):\n",
    "    return [(ent['text'] if text_only else ent)\n",
    "            for ent in ent_list\n",
    "            if ent['label'] == label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to figure out if the entities a tweet contains include known news/relief organisations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_news_mention(ent_list):\n",
    "    return bool(select_ents_by_label('NEWS-ORG', ent_list))\n",
    "\n",
    "def contains_relief_mention(ent_list):\n",
    "    return bool(select_ents_by_label('RELIEF-ORG', ent_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the dataset\n",
    "\n",
    "Next, create a new dataframe and run the pipeline + aux functions on the whole training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extraction: 100%|██████████| 7503/7503 [04:02<00:00, 30.97it/s]\n"
     ]
    }
   ],
   "source": [
    "df_out = pd.DataFrame(columns=['id', 'keyword', 'location', 'text', 'hashtag', 'hashtags_have_l1_synonyms', 'hashtags_have_l2_synonyms', 'subj', 'verb', 'obj', 'contains_l1_synonyms', 'contains_l2_synonyms', 'contains_damaged_words', 'mentioned_news_org', 'mentioned_relief_org', 'mentions', 'orgs', 'gpes', 'facs', 'target'])\n",
    "\n",
    "from tqdm import tqdm\n",
    "for i, twit in tqdm(df.iterrows(), total=df.shape[0], desc=\"Feature extraction\"):\n",
    "    doc = nlp(twit.text)\n",
    "    useful_tokens = get_useful_tokens(doc)\n",
    "    useful_entities = get_useful_ents(doc)\n",
    "    \n",
    "    subjs, objs, verbs = extract_sov(useful_tokens)\n",
    "    \n",
    "    l1_synonyms = {'accident', 'accidentally', 'accidental', \n",
    "                   'terrible', 'terribly',\n",
    "                   'unfortunate',\n",
    "                   'calamity', 'cµµalamitous', \n",
    "                   'catastrophe', 'catastrophic', 'catastrophically', \n",
    "                   'collapse', \n",
    "                   'crash', \n",
    "                   'debacle', \n",
    "                   'defeat',\n",
    "                   'drown',\n",
    "                   'emergency', \n",
    "                   'failure', \n",
    "                   'fiasco', \n",
    "                   'flood', \n",
    "                   'harm', 'harmful', 'harmfully', \n",
    "                   'hazard', 'hazardous', 'hazardously', \n",
    "                   'holocaust', \n",
    "                   'mishap', \n",
    "                   'setback', \n",
    "                   'tragedy', 'tragic', 'tragically'}\n",
    "    l2_synonyms = {'adversity', 'adverse', 'adversely', \n",
    "                   'affliction', \n",
    "                   'bale', \n",
    "                   'bane', \n",
    "                   'blight', \n",
    "                   'blow', \n",
    "                   'bust', \n",
    "                   'casualty', \n",
    "                   'cataclysm', 'cataclysmic', 'cataclysmically', \n",
    "                   'collision', \n",
    "                   'depression', \n",
    "                   'exigency', \n",
    "                   'fall', \n",
    "                   'flop', \n",
    "                   'grief', \n",
    "                   'misadventure', \n",
    "                   'mischance', \n",
    "                   'misfortune', \n",
    "                   'reverse',\n",
    "                   'rock', \n",
    "                   'rough', \n",
    "                   'ruin', 'ruination', \n",
    "                   'slip', \n",
    "                   'stroke', \n",
    "                   'undoing', \n",
    "                   'upset', \n",
    "                   'woe', 'woeful', 'woefully', \n",
    "                   'sad', 'sadness', 'sadly'}\n",
    "    \n",
    "    contains_l1_synonyms = 0 < len(l1_synonyms.intersection({x['token'] for x in useful_tokens}))\n",
    "    contains_l2_synonyms = 0 < len(l2_synonyms.intersection({x['token'] for x in useful_tokens}))\n",
    "    hashtags_have_l1_synonyms = 0 < len(l1_synonyms.intersection({x[1:] for x in select_ents_by_label('HASHTAG', useful_entities, text_only=True)}))\n",
    "    hashtags_have_l2_synonyms = 0 < len(l2_synonyms.intersection({x[1:] for x in select_ents_by_label('HASHTAG', useful_entities, text_only=True)}))\n",
    "    \n",
    "    damaged_words = {\n",
    "        'crack',\n",
    "        'rupture',\n",
    "        'fissure',\n",
    "        'burn',\n",
    "        'combust',\n",
    "        'explode', 'explosion',\n",
    "        'implode', 'implosion',\n",
    "        'corrode', 'corrosion',\n",
    "        'rust',\n",
    "        'flood',\n",
    "        'pollute', 'pollution',\n",
    "        'radioactive', 'radioactivity',\n",
    "        'meltdown',\n",
    "        'splinter',\n",
    "        'disintegrate',\n",
    "        'collapse',\n",
    "        'sink',\n",
    "        'shatter',\n",
    "        'bruise'\n",
    "    } # ways in which shit can be broken\n",
    "    \n",
    "    contains_damaged_words = 0 < len(damaged_words.intersection({x['token'] for x in useful_tokens}))\n",
    "    \n",
    "    df_out.loc[i] = {\n",
    "        'id': twit.id,\n",
    "        'keyword': twit.keyword,\n",
    "        'location': twit.location,\n",
    "        'text': twit.text,\n",
    "        'hashtag': select_ents_by_label('HASHTAG', useful_entities, text_only=True),\n",
    "        'hashtags_have_l1_synonyms': hashtags_have_l1_synonyms,\n",
    "        'hashtags_have_l2_synonyms': hashtags_have_l2_synonyms,\n",
    "        'subj': subjs,\n",
    "        'verb': verbs,\n",
    "        'obj': objs,\n",
    "        'mentioned_news_org': contains_news_mention(useful_entities),\n",
    "        'mentioned_relief_org': contains_relief_mention(useful_entities),\n",
    "        'mentions': select_ents_by_label('MENTION', useful_entities, text_only=True),\n",
    "        'contains_l1_synonyms': contains_l1_synonyms,\n",
    "        'contains_l2_synonyms': contains_l2_synonyms,\n",
    "        'contains_damaged_words': contains_damaged_words,\n",
    "        'orgs': select_ents_by_label('ORG', useful_entities, text_only=True),\n",
    "        'gpes': select_ents_by_label('GPE', useful_entities, text_only=True),\n",
    "        'facs': select_ents_by_label('FAC', useful_entities, text_only=True),\n",
    "        'target': twit.target\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text', 'hashtag',\n",
       "       'hashtags_have_l1_synonyms', 'hashtags_have_l2_synonyms', 'subj',\n",
       "       'verb', 'obj', 'contains_l1_synonyms', 'contains_l2_synonyms',\n",
       "       'contains_damaged_words', 'mentioned_news_org', 'mentioned_relief_org',\n",
       "       'mentions', 'orgs', 'gpes', 'facs', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace empty lists with a placeholder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_out.applymap(lambda cell: ['_'] if (type(cell) == list and len(cell) == 0) else cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the extracted features dataframe to a parquet file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_parquet(\"features.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: useful functions\n",
    "\n",
    "Extract all mentions from the dataframe into a text file for further processing/analyzing:\n",
    "\n",
    "```python\n",
    "with open('mentions.txt', \"w\") as f:\n",
    "    for i,content in df[df.mentions.map(bool)].iterrows():\n",
    "        [f.write(\"%s\\n\" % x) for x in content.mentions]\n",
    "``` "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
