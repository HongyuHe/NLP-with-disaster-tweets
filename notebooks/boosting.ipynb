{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "      <th>labels</th>\n",
       "      <th>ent_dep</th>\n",
       "      <th>ent_head</th>\n",
       "      <th>ent_pos</th>\n",
       "      <th>ent_children</th>\n",
       "      <th>...</th>\n",
       "      <th>contains_l1_synonyms</th>\n",
       "      <th>contains_l2_synonyms</th>\n",
       "      <th>contains_damaged_words</th>\n",
       "      <th>mentioned_news_org</th>\n",
       "      <th>mentioned_relief_org</th>\n",
       "      <th>mentions</th>\n",
       "      <th>orgs</th>\n",
       "      <th>gpes</th>\n",
       "      <th>facs</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>31</td>\n",
       "      <td>missing</td>\n",
       "      <td>unknown</td>\n",
       "      <td>this is ridiculous....</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>33</td>\n",
       "      <td>missing</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Love skiing</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>36</td>\n",
       "      <td>missing</td>\n",
       "      <td>unknown</td>\n",
       "      <td>LOOOOOOL</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>38</td>\n",
       "      <td>missing</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Was in NYC last week!</td>\n",
       "      <td>NYC,last week</td>\n",
       "      <td>ORG,DATE</td>\n",
       "      <td>pobj</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>['NYC']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>40</td>\n",
       "      <td>missing</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Cooool :)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  keyword location                    text       entities    labels  \\\n",
       "20  31  missing  unknown  this is ridiculous....           None      None   \n",
       "22  33  missing  unknown             Love skiing           None      None   \n",
       "24  36  missing  unknown                LOOOOOOL           None      None   \n",
       "26  38  missing  unknown   Was in NYC last week!  NYC,last week  ORG,DATE   \n",
       "28  40  missing  unknown               Cooool :)           None      None   \n",
       "\n",
       "   ent_dep ent_head ent_pos ent_children  ... contains_l1_synonyms  \\\n",
       "20    None     None    None         None  ...                False   \n",
       "22    None     None    None         None  ...                False   \n",
       "24    None     None    None         None  ...                False   \n",
       "26    pobj       in     ADP         None  ...                False   \n",
       "28    None     None    None         None  ...                False   \n",
       "\n",
       "    contains_l2_synonyms  contains_damaged_words mentioned_news_org  \\\n",
       "20                 False                   False              False   \n",
       "22                 False                   False              False   \n",
       "24                 False                   False              False   \n",
       "26                 False                   False              False   \n",
       "28                 False                   False              False   \n",
       "\n",
       "   mentioned_relief_org mentions     orgs  gpes  facs  target  \n",
       "20                False       []       []    []    []     0.0  \n",
       "22                False       []       []    []    []     0.0  \n",
       "24                False       []       []    []    []     0.0  \n",
       "26                False       []  ['NYC']    []    []     0.0  \n",
       "28                False       []       []    []    []     0.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# df = pd.read_csv('../datasets/entities_nlp_train_hongyu.csv')\n",
    "df = pd.read_csv('../datasets/all-nlp-features.csv')\n",
    "df[20:30:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7503,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['target']\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text', 'entities', 'labels', 'ent_dep',\n",
       "       'ent_head', 'ent_pos', 'ent_children', 'hashtag',\n",
       "       'hashtags_have_l1_synonyms', 'hashtags_have_l2_synonyms', 'subj',\n",
       "       'verb', 'obj', 'contains_l1_synonyms', 'contains_l2_synonyms',\n",
       "       'contains_damaged_words', 'mentioned_news_org', 'mentioned_relief_org',\n",
       "       'mentions', 'orgs', 'gpes', 'facs'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(['target'], axis=1)\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "class LabeledNormalizer(Normalizer):\n",
    "    def fit(self, X, *args, **kwargs):\n",
    "        try:\n",
    "            self.names = X.columns\n",
    "        except:\n",
    "            self.names = [str(i) for i in range(X.shape[1])]\n",
    "        return super().fit(X, *args, **kwargs)\n",
    "         \n",
    "    def get_feature_names(self):\n",
    "        return self.names\n",
    "    \n",
    "vec = ColumnTransformer([\n",
    "#     ('norm', LabeledNormalizer(), ['id']),\n",
    "    ('kw', TfidfVectorizer(ngram_range=(1, 3), min_df=2, token_pattern=r\"(?u)\\b\\w+\\b\",), 'keyword'),\n",
    "    ('loc', TfidfVectorizer(ngram_range=(1, 3), min_df=2, token_pattern=r\"(?u)\\b\\w+\\b\"), 'location'),\n",
    "    ('text', TfidfVectorizer(ngram_range=(1, 3), min_df=2, token_pattern=r\"(?u)\\b\\w+\\b\"), 'text'),\n",
    "    ('ent', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'entities'),\n",
    "    ('label', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'labels'),\n",
    "    ('dep', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'ent_dep'),\n",
    "    ('head', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'ent_head'),\n",
    "    ('pos', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'ent_pos'),\n",
    "    ('child', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'ent_children'),\n",
    "    \n",
    "])\n",
    "\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.svm import SVC\n",
    "# clf = SVC(kernel='linear', probability=True)\n",
    "# clf = SVC(kernel='rbf', probability=True)\n",
    "# clf = SVC(kernel='poly', probability=True)\n",
    "# clf = LinearSVC(verbose=True)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# clf = RandomForestClassifier(n_estimators=20, random_state=0) # use a guassian forest\n",
    "clf = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "# clf = RandomForestClassifier(n_estimators=500, random_state=0) # parallel all jobs\n",
    "\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler(with_mean=False)\n",
    "\n",
    "pipeline = make_pipeline(vec, scaler, clf)\n",
    "\n",
    "# pipeline = make_pipeline(vec, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "class LabeledNormalizer(Normalizer):\n",
    "    def fit(self, X, *args, **kwargs):\n",
    "        try:\n",
    "            self.names = X.columns\n",
    "        except:\n",
    "            self.names = [str(i) for i in range(X.shape[1])]\n",
    "        return super().fit(X, *args, **kwargs)\n",
    "         \n",
    "    def get_feature_names(self):\n",
    "        return self.names\n",
    "    \n",
    "class SupervisionFriendlyLabelBinarizer(LabelBinarizer):\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return super(SupervisionFriendlyLabelBinarizer,self).fit_transform(X)\n",
    "\n",
    "class MultiLabelBinarizerWrapper(MultiLabelBinarizer):\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return super(MultiLabelBinarizerWrapper,self).fit_transform(X)\n",
    "    def get_params(self, deep=True):\n",
    "        return super(MultiLabelBinarizer,self).get_params(deep=True)\n",
    "\n",
    "# enc = OneHotEncoder(handle_unknown='ignore')\n",
    "# lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "vec = ColumnTransformer([\n",
    "#     ('norm', LabeledNormalizer(), ['id']),\n",
    "    ('kw', TfidfVectorizer(ngram_range=(1, 1), min_df=2, token_pattern=r\"(?u)\\b\\w+\\b\",), 'keyword'),\n",
    "    ('loc', TfidfVectorizer(ngram_range=(1, 1), min_df=2, token_pattern=r\"(?u)\\b\\w+\\b\"), 'location'),\n",
    "    ('text', TfidfVectorizer(ngram_range=(1, 3), min_df=2, token_pattern=r\"(?u)\\b\\w+\\b\"), 'text'),\n",
    "    ('ent', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'entities'),\n",
    "    ('label', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'labels'),\n",
    "    ('dep', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'ent_dep'),\n",
    "    ('head', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'ent_head'),\n",
    "    ('pos', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'ent_pos'),\n",
    "    ('child', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'ent_children'),\n",
    "#     ('hash', MultiLabelBinarizerWrapper(), 'hashtag'),\n",
    "#     ('hashl1', SupervisionFriendlyLabelBinarizer(), 'hashtags_have_l1_synonyms'),\n",
    "#     ('hashl2', SupervisionFriendlyLabelBinarizer(), 'hashtags_have_l2_synonyms'),\n",
    "#     ('subj', MultiLabelBinarizerWrapper(), 'subj'),\n",
    "#     ('obj', MultiLabelBinarizerWrapper(), 'verb'),\n",
    "#     ('verb', MultiLabelBinarizerWrapper(), 'obj'),\n",
    "#     ('syn1', SupervisionFriendlyLabelBinarizer(), 'contains_l1_synonyms'),\n",
    "#     ('syn2', SupervisionFriendlyLabelBinarizer(), 'contains_l2_synonyms'),\n",
    "#     ('damage', SupervisionFriendlyLabelBinarizer(), 'contains_damaged_words'),\n",
    "#     ('news', SupervisionFriendlyLabelBinarizer(), 'mentioned_news_org'),\n",
    "#     ('relief', SupervisionFriendlyLabelBinarizer(), 'mentioned_relief_org'),\n",
    "#     ('mentions', MultiLabelBinarizerWrapper(), 'mentions'),\n",
    "#     ('orgs', MultiLabelBinarizerWrapper, 'orgs'),\n",
    "#     ('gpes', MultiLabelBinarizerWrapper, 'gpes'),\n",
    "#     ('facs', MultiLabelBinarizerWrapper, 'facs'),\n",
    "    \n",
    "])\n",
    "\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# poly = PolynomialFeatures(2)\n",
    "\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.svm import SVC\n",
    "# clf = SVC(kernel='linear', probability=True)\n",
    "# clf = SVC(kernel='rbf', probability=True)\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# clf = RandomForestClassifier(n_estimators=20, random_state=0) # use a guassian forest\n",
    "# clf = RandomForestClassifier(n_estimators=500, random_state=0) # set limit to prevent overfitting\n",
    "# clf = RandomForestClassifier(n_estimators=200, max_features=1000, random_state=0) # set limit to prevent overfitting\n",
    "\n",
    "scaler = preprocessing.StandardScaler(with_mean=False)\n",
    "\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# clf = MLPClassifier(solver='lbfgs', alpha=1e-5, activation='relu', hidden_layer_sizes=(500, 500, 500), max_iter=2000, random_state=1)\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=500, random_state=0)\n",
    "\n",
    "pipeline = make_pipeline(vec, scaler, clf)\n",
    "# pipeline = make_pipeline(vec, clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.70      0.68      4305\n",
      "         1.0       0.56      0.52      0.54      3198\n",
      "\n",
      "    accuracy                           0.62      7503\n",
      "   macro avg       0.61      0.61      0.61      7503\n",
      "weighted avg       0.62      0.62      0.62      7503\n",
      "\n",
      "Cross-validation MSE: 0.620 ± 0.072\n",
      "Training Set Accuracy: 0.894\n",
      "\n",
      "Evaluation Time Taken:  00:03:45\n",
      "Training Time Taken:   00:00:26\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "def evaluate(_clf, X, y):\n",
    "    report = classification_report(\n",
    "        y_true=y, y_pred=cross_val_predict(pipeline, X, y, cv=5)\n",
    "    )\n",
    "    print(report)\n",
    "    scores = cross_val_score(_clf, X, y, scoring='accuracy', cv=5)\n",
    "    print('Cross-validation MSE: {:.3f} ± {:.3f}'.format(np.mean(scores), 2 * np.std(scores)))\n",
    "    \n",
    "    _clf.fit(X,y)\n",
    "    print('Training Set Accuracy: {:.3f}'.format(_clf.score(X,y)))\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "evaluate(pipeline, X, y)\n",
    "\n",
    "seconds = time.time() - start_time\n",
    "print('\\nEvaluation Time Taken: ', time.strftime(\"%H:%M:%S\",time.gmtime(seconds)))\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "pipeline.fit(X,y)\n",
    "\n",
    "seconds = time.time() - start_time\n",
    "print('Training Time Taken:  ', time.strftime(\"%H:%M:%S\",time.gmtime(seconds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "### 21 features 100 ensemble\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.65      0.69      0.67      4305\n",
    "         1.0       0.54      0.49      0.51      3198\n",
    "\n",
    "    accuracy                           0.61      7503\n",
    "   macro avg       0.59      0.59      0.59      7503\n",
    "weighted avg       0.60      0.61      0.60      7503\n",
    "\n",
    "Cross-validation MSE: 0.606 ± 0.078\n",
    "Training Set Accuracy: 0.898\n",
    "\n",
    "Evaluation Time Taken:  00:03:08\n",
    "Training Time Taken:   00:00:18\n",
    "```\n",
    "\n",
    "### 21 features 100 ensemble\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.66      0.79      0.72      4305\n",
    "         1.0       0.63      0.46      0.53      3198\n",
    "\n",
    "    accuracy                           0.65      7503\n",
    "   macro avg       0.65      0.63      0.63      7503\n",
    "weighted avg       0.65      0.65      0.64      7503\n",
    "\n",
    "Cross-validation MSE: 0.653 ± 0.083\n",
    "Training Set Accuracy: 0.798\n",
    "\n",
    "Evaluation Time Taken:  00:00:43\n",
    "Training Time Taken:   00:00:04\n",
    "```\n",
    "\n",
    "## Adaboost with entity features (100 ensemble)\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.66      0.79      0.72      4305\n",
    "         1.0       0.61      0.44      0.51      3198\n",
    "\n",
    "    accuracy                           0.64      7503\n",
    "   macro avg       0.63      0.62      0.61      7503\n",
    "weighted avg       0.64      0.64      0.63      7503\n",
    "\n",
    "Cross-validation MSE: 0.642 ± 0.083\n",
    "Training Set Accuracy: 0.801\n",
    "\n",
    "Evaluation Time Taken:  00:00:33\n",
    "Training Time Taken:   00:00:03\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with 21 features\n",
    "\n",
    "### Params: hidden_layer_sizes=(500, 500, 500), max_iter=2000\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.72      0.69      0.70      4305\n",
    "         1.0       0.60      0.64      0.62      3198\n",
    "\n",
    "    accuracy                           0.67      7503\n",
    "   macro avg       0.66      0.66      0.66      7503\n",
    "weighted avg       0.67      0.67      0.67      7503\n",
    "\n",
    "Cross-validation MSE: 0.665 ± 0.059\n",
    "Training Set Accuracy: 0.998\n",
    "\n",
    "Evaluation Time Taken:  09:23:30\n",
    "Training Time Taken:   00:19:10\n",
    "```\n",
    "\n",
    "### Params: `solver='lbfgs', alpha=1e-5, activation='tanh', hidden_layer_sizes=(100, 100, 100), max_iter=1000`\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.72      0.64      0.68      4305\n",
    "         1.0       0.58      0.66      0.62      3198\n",
    "\n",
    "    accuracy                           0.65      7503\n",
    "   macro avg       0.65      0.65      0.65      7503\n",
    "weighted avg       0.66      0.65      0.65      7503\n",
    "\n",
    "Cross-validation MSE: 0.649 ± 0.090\n",
    "Training Set Accuracy: 0.998\n",
    "\n",
    "Evaluation Time Taken:  00:47:30\n",
    "Training Time Taken:   00:05:01\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 hongyu hongyu 154M Mar 29 10:09 ../models/MLP-21-features.pkl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# pipeline.fit(X, y)\n",
    "\n",
    "mlp_mdl = '../models/MLP-21-features.pkl'\n",
    "joblib.dump(pipeline, mlp_mdl)\n",
    "!ls -lSh $mlp_mdl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
