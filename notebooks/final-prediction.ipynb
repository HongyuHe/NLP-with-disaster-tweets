{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "      <th>labels</th>\n",
       "      <th>ent_dep</th>\n",
       "      <th>ent_head</th>\n",
       "      <th>ent_pos</th>\n",
       "      <th>ent_children</th>\n",
       "      <th>...</th>\n",
       "      <th>contains_l1_synonyms</th>\n",
       "      <th>contains_l2_synonyms</th>\n",
       "      <th>contains_damaged_words</th>\n",
       "      <th>mentioned_news_org</th>\n",
       "      <th>mentioned_relief_org</th>\n",
       "      <th>mentions</th>\n",
       "      <th>orgs</th>\n",
       "      <th>gpes</th>\n",
       "      <th>facs</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>31</td>\n",
       "      <td>missing</td>\n",
       "      <td>unknown</td>\n",
       "      <td>this is ridiculous....</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>33</td>\n",
       "      <td>missing</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Love skiing</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>36</td>\n",
       "      <td>missing</td>\n",
       "      <td>unknown</td>\n",
       "      <td>LOOOOOOL</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>38</td>\n",
       "      <td>missing</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Was in NYC last week!</td>\n",
       "      <td>NYC,last week</td>\n",
       "      <td>ORG,DATE</td>\n",
       "      <td>pobj</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>['NYC']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>40</td>\n",
       "      <td>missing</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Cooool :)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  keyword location                    text       entities    labels  \\\n",
       "20  31  missing  unknown  this is ridiculous....           None      None   \n",
       "22  33  missing  unknown             Love skiing           None      None   \n",
       "24  36  missing  unknown                LOOOOOOL           None      None   \n",
       "26  38  missing  unknown   Was in NYC last week!  NYC,last week  ORG,DATE   \n",
       "28  40  missing  unknown               Cooool :)           None      None   \n",
       "\n",
       "   ent_dep ent_head ent_pos ent_children  ... contains_l1_synonyms  \\\n",
       "20    None     None    None         None  ...                False   \n",
       "22    None     None    None         None  ...                False   \n",
       "24    None     None    None         None  ...                False   \n",
       "26    pobj       in     ADP         None  ...                False   \n",
       "28    None     None    None         None  ...                False   \n",
       "\n",
       "    contains_l2_synonyms  contains_damaged_words mentioned_news_org  \\\n",
       "20                 False                   False              False   \n",
       "22                 False                   False              False   \n",
       "24                 False                   False              False   \n",
       "26                 False                   False              False   \n",
       "28                 False                   False              False   \n",
       "\n",
       "   mentioned_relief_org mentions     orgs  gpes  facs  target  \n",
       "20                False       []       []    []    []     0.0  \n",
       "22                False       []       []    []    []     0.0  \n",
       "24                False       []       []    []    []     0.0  \n",
       "26                False       []  ['NYC']    []    []     0.0  \n",
       "28                False       []       []    []    []     0.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# df = pd.read_csv('../datasets/entities_nlp_train_hongyu.csv')\n",
    "df = pd.read_csv('../datasets/all-nlp-features.csv')\n",
    "df[20:30:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7503,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['target']\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text', 'entities', 'labels', 'ent_dep',\n",
       "       'ent_head', 'ent_pos', 'ent_children', 'hashtag',\n",
       "       'hashtags_have_l1_synonyms', 'hashtags_have_l2_synonyms', 'subj',\n",
       "       'verb', 'obj', 'contains_l1_synonyms', 'contains_l2_synonyms',\n",
       "       'contains_damaged_words', 'mentioned_news_org', 'mentioned_relief_org',\n",
       "       'mentions', 'orgs', 'gpes', 'facs'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(['target'], axis=1)\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "class LabeledNormalizer(Normalizer):\n",
    "    def fit(self, X, *args, **kwargs):\n",
    "        try:\n",
    "            self.names = X.columns\n",
    "        except:\n",
    "            self.names = [str(i) for i in range(X.shape[1])]\n",
    "        return super().fit(X, *args, **kwargs)\n",
    "         \n",
    "    def get_feature_names(self):\n",
    "        return self.names\n",
    "    \n",
    "vec = ColumnTransformer([\n",
    "#     ('norm', LabeledNormalizer(), ['id']),\n",
    "    ('kw', TfidfVectorizer(ngram_range=(1, 3), min_df=2, token_pattern=r\"(?u)\\b\\w+\\b\",), 'keyword'),\n",
    "    ('loc', TfidfVectorizer(ngram_range=(1, 3), min_df=2, token_pattern=r\"(?u)\\b\\w+\\b\"), 'location'),\n",
    "    ('text', TfidfVectorizer(ngram_range=(1, 3), min_df=2, token_pattern=r\"(?u)\\b\\w+\\b\"), 'text'),\n",
    "    ('ent', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'entities'),\n",
    "    ('label', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'labels'),\n",
    "    ('dep', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'ent_dep'),\n",
    "    ('head', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'ent_head'),\n",
    "    ('pos', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'ent_pos'),\n",
    "    ('child', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'ent_children'),\n",
    "    \n",
    "])\n",
    "\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.svm import SVC\n",
    "# clf = SVC(kernel='linear', probability=True)\n",
    "# clf = SVC(kernel='rbf', probability=True)\n",
    "# clf = SVC(kernel='poly', probability=True)\n",
    "# clf = LinearSVC(verbose=True)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# clf = RandomForestClassifier(n_estimators=20, random_state=0) # use a guassian forest\n",
    "clf = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "# clf = RandomForestClassifier(n_estimators=500, random_state=0) # parallel all jobs\n",
    "\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler(with_mean=False)\n",
    "\n",
    "pipeline = make_pipeline(vec, scaler, clf)\n",
    "\n",
    "# pipeline = make_pipeline(vec, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "class LabeledNormalizer(Normalizer):\n",
    "    def fit(self, X, *args, **kwargs):\n",
    "        try:\n",
    "            self.names = X.columns\n",
    "        except:\n",
    "            self.names = [str(i) for i in range(X.shape[1])]\n",
    "        return super().fit(X, *args, **kwargs)\n",
    "         \n",
    "    def get_feature_names(self):\n",
    "        return self.names\n",
    "    \n",
    "class SupervisionFriendlyLabelBinarizer(LabelBinarizer):\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return super(SupervisionFriendlyLabelBinarizer,self).fit_transform(X)\n",
    "\n",
    "class MultiLabelBinarizerWrapper(MultiLabelBinarizer):\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return super(MultiLabelBinarizerWrapper,self).fit_transform(X)\n",
    "    def get_params(self, deep=True):\n",
    "        return super(MultiLabelBinarizer,self).get_params(deep=True)\n",
    "\n",
    "# enc = OneHotEncoder(handle_unknown='ignore')\n",
    "# lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "vec = ColumnTransformer([\n",
    "#     ('norm', LabeledNormalizer(), ['id']),\n",
    "    ('kw', TfidfVectorizer(ngram_range=(1, 1), min_df=2, token_pattern=r\"(?u)\\b\\w+\\b\",), 'keyword'),\n",
    "    ('loc', TfidfVectorizer(ngram_range=(1, 1), min_df=2, token_pattern=r\"(?u)\\b\\w+\\b\"), 'location'),\n",
    "    ('text', TfidfVectorizer(ngram_range=(1, 3), min_df=2, token_pattern=r\"(?u)\\b\\w+\\b\"), 'text'),\n",
    "    ('ent', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'entities'),\n",
    "    ('label', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'labels'),\n",
    "    ('dep', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'ent_dep'),\n",
    "    ('head', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'ent_head'),\n",
    "    ('pos', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'ent_pos'),\n",
    "    ('child', CountVectorizer(ngram_range=(1, 1), analyzer='word', token_pattern=r\"(?u)\\b\\w+\\b\"), 'ent_children'),\n",
    "    ('hash', MultiLabelBinarizerWrapper(), 'hashtag'),\n",
    "    ('hashl1', SupervisionFriendlyLabelBinarizer(), 'hashtags_have_l1_synonyms'),\n",
    "    ('hashl2', SupervisionFriendlyLabelBinarizer(), 'hashtags_have_l2_synonyms'),\n",
    "    ('subj', MultiLabelBinarizerWrapper(), 'subj'),\n",
    "    ('obj', MultiLabelBinarizerWrapper(), 'verb'),\n",
    "    ('verb', MultiLabelBinarizerWrapper(), 'obj'),\n",
    "    ('syn1', SupervisionFriendlyLabelBinarizer(), 'contains_l1_synonyms'),\n",
    "    ('syn2', SupervisionFriendlyLabelBinarizer(), 'contains_l2_synonyms'),\n",
    "    ('damage', SupervisionFriendlyLabelBinarizer(), 'contains_damaged_words'),\n",
    "    ('news', SupervisionFriendlyLabelBinarizer(), 'mentioned_news_org'),\n",
    "    ('relief', SupervisionFriendlyLabelBinarizer(), 'mentioned_relief_org'),\n",
    "    ('mentions', MultiLabelBinarizerWrapper(), 'mentions'),\n",
    "#     ('orgs', MultiLabelBinarizerWrapper, 'orgs'),\n",
    "#     ('gpes', MultiLabelBinarizerWrapper, 'gpes'),\n",
    "#     ('facs', MultiLabelBinarizerWrapper, 'facs'),\n",
    "    \n",
    "])\n",
    "\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# poly = PolynomialFeatures(2)\n",
    "\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.svm import SVC\n",
    "# clf = SVC(kernel='linear', probability=True)\n",
    "# clf = SVC(kernel='rbf', probability=True)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# clf = RandomForestClassifier(n_estimators=20, random_state=0) # use a guassian forest\n",
    "# clf = RandomForestClassifier(n_estimators=500, random_state=0) # set limit to prevent overfitting\n",
    "clf = RandomForestClassifier(n_estimators=250,min_samples_split=6,min_samples_leaf=4, max_features='sqrt',max_depth=80,bootstrap=True) # set limit to prevent overfitting\n",
    "\n",
    "scaler = preprocessing.StandardScaler(with_mean=False)\n",
    "\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# clf = MLPClassifier(solver='lbfgs', alpha=1e-5, activation='relu', hidden_layer_sizes=(500, 500, 500), max_iter=2000, random_state=1)\n",
    "\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# clf = AdaBoostClassifier(n_estimators=500, random_state=0)\n",
    "\n",
    "pipeline = make_pipeline(vec, scaler, clf)\n",
    "# pipeline = make_pipeline(vec, clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time Taken:   00:00:05\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "pipeline.fit(X,y)\n",
    "\n",
    "seconds = time.time() - start_time\n",
    "print('Training Time Taken:  ', time.strftime(\"%H:%M:%S\",time.gmtime(seconds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "      <th>labels</th>\n",
       "      <th>ent_dep</th>\n",
       "      <th>ent_head</th>\n",
       "      <th>ent_pos</th>\n",
       "      <th>ent_children</th>\n",
       "      <th>...</th>\n",
       "      <th>obj</th>\n",
       "      <th>contains_l1_synonyms</th>\n",
       "      <th>contains_l2_synonyms</th>\n",
       "      <th>contains_damaged_words</th>\n",
       "      <th>mentioned_news_org</th>\n",
       "      <th>mentioned_relief_org</th>\n",
       "      <th>mentions</th>\n",
       "      <th>orgs</th>\n",
       "      <th>gpes</th>\n",
       "      <th>facs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>missing</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>['crash']</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>['_']</td>\n",
       "      <td>['terrible']</td>\n",
       "      <td>['_']</td>\n",
       "      <td>['_']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>missing</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>about #earthquake</td>\n",
       "      <td>MONEY</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>['_']</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>['_']</td>\n",
       "      <td>['different' 'everyone']</td>\n",
       "      <td>['_']</td>\n",
       "      <td>['_']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>missing</td>\n",
       "      <td>unknown</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>geese</td>\n",
       "      <td>NORP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>fleeing</td>\n",
       "      <td>VERB</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>['pond' 'street']</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>['_']</td>\n",
       "      <td>['there']</td>\n",
       "      <td>['_']</td>\n",
       "      <td>['_']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>missing</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>['_']</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>['_']</td>\n",
       "      <td>['_']</td>\n",
       "      <td>['Apocalypse']</td>\n",
       "      <td>['_']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>missing</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>Typhoon Soudelor,28,China,Taiwan</td>\n",
       "      <td>ORG,CARDINAL,GPE</td>\n",
       "      <td>dobj,pobj,conj</td>\n",
       "      <td>kills,in,China</td>\n",
       "      <td>VERB,ADP,PROPN</td>\n",
       "      <td>and,Taiwan</td>\n",
       "      <td>...</td>\n",
       "      <td>['china' '28']</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>['_']</td>\n",
       "      <td>['Typhoon Soudelor']</td>\n",
       "      <td>['China' 'Taiwan']</td>\n",
       "      <td>['_']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  keyword location                                               text  \\\n",
       "0   0  missing  unknown                 Just happened a terrible car crash   \n",
       "1   2  missing  unknown  Heard about #earthquake is different cities, s...   \n",
       "2   3  missing  unknown  there is a forest fire at spot pond, geese are...   \n",
       "3   9  missing  unknown           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11  missing  unknown      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                           entities            labels         ent_dep  \\\n",
       "0                              None              None            None   \n",
       "1                 about #earthquake             MONEY            None   \n",
       "2                             geese              NORP           nsubj   \n",
       "3                              None              None            None   \n",
       "4  Typhoon Soudelor,28,China,Taiwan  ORG,CARDINAL,GPE  dobj,pobj,conj   \n",
       "\n",
       "         ent_head         ent_pos ent_children  ...                obj  \\\n",
       "0            None            None         None  ...          ['crash']   \n",
       "1            None            None         None  ...              ['_']   \n",
       "2         fleeing            VERB         None  ...  ['pond' 'street']   \n",
       "3            None            None         None  ...              ['_']   \n",
       "4  kills,in,China  VERB,ADP,PROPN   and,Taiwan  ...     ['china' '28']   \n",
       "\n",
       "   contains_l1_synonyms  contains_l2_synonyms contains_damaged_words  \\\n",
       "0                  True                 False                  False   \n",
       "1                 False                 False                  False   \n",
       "2                 False                 False                  False   \n",
       "3                 False                 False                  False   \n",
       "4                 False                 False                  False   \n",
       "\n",
       "  mentioned_news_org mentioned_relief_org  mentions                      orgs  \\\n",
       "0              False                False     ['_']              ['terrible']   \n",
       "1              False                False     ['_']  ['different' 'everyone']   \n",
       "2              False                False     ['_']                 ['there']   \n",
       "3              False                False     ['_']                     ['_']   \n",
       "4              False                False     ['_']      ['Typhoon Soudelor']   \n",
       "\n",
       "                 gpes   facs  \n",
       "0               ['_']  ['_']  \n",
       "1               ['_']  ['_']  \n",
       "2               ['_']  ['_']  \n",
       "3      ['Apocalypse']  ['_']  \n",
       "4  ['China' 'Taiwan']  ['_']  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('../datasets/test-features.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:987: UserWarning: unknown class(es) ['(', ':', 'Ò'] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:987: UserWarning: unknown class(es) ['%', '+', '?'] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:987: UserWarning: unknown class(es) ['(', 'ì'] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:987: UserWarning: unknown class(es) ['ä'] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    }
   ],
   "source": [
    "predictions = pipeline.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.array(predictions, dtype=[('target', 'int')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target= pd.DataFrame(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target.shape\n",
    "# df_test['id'].join(target)\n",
    "results = test_df.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       0\n",
       "1         2       0\n",
       "2         3       0\n",
       "3         9       0\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       0\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       0\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results = results[['id', 'target']].copy()\n",
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 hongyu hongyu 14M Mar 30 00:16 ../models/FINAL-random-forest-tuned.pkl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# pipeline.fit(X, y)\n",
    "final_mdl = '../models/FINAL-random-forest-tuned.pkl'\n",
    "joblib.dump(pipeline, final_mdl)\n",
    "!ls -lSh $final_mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results.to_csv('../predictions.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.70      0.68      4305\n",
      "         1.0       0.56      0.52      0.54      3198\n",
      "\n",
      "    accuracy                           0.62      7503\n",
      "   macro avg       0.61      0.61      0.61      7503\n",
      "weighted avg       0.62      0.62      0.62      7503\n",
      "\n",
      "Cross-validation MSE: 0.620 ± 0.072\n",
      "Training Set Accuracy: 0.894\n",
      "\n",
      "Evaluation Time Taken:  00:03:45\n",
      "Training Time Taken:   00:00:26\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "def evaluate(_clf, X, y):\n",
    "    report = classification_report(\n",
    "        y_true=y, y_pred=cross_val_predict(pipeline, X, y, cv=5)\n",
    "    )\n",
    "    print(report)\n",
    "    scores = cross_val_score(_clf, X, y, scoring='accuracy', cv=5)\n",
    "    print('Cross-validation MSE: {:.3f} ± {:.3f}'.format(np.mean(scores), 2 * np.std(scores)))\n",
    "    \n",
    "    _clf.fit(X,y)\n",
    "    print('Training Set Accuracy: {:.3f}'.format(_clf.score(X,y)))\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "evaluate(pipeline, X, y)\n",
    "\n",
    "seconds = time.time() - start_time\n",
    "print('\\nEvaluation Time Taken: ', time.strftime(\"%H:%M:%S\",time.gmtime(seconds)))\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "pipeline.fit(X,y)\n",
    "\n",
    "seconds = time.time() - start_time\n",
    "print('Training Time Taken:  ', time.strftime(\"%H:%M:%S\",time.gmtime(seconds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "### 21 features 100 ensemble\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.65      0.69      0.67      4305\n",
    "         1.0       0.54      0.49      0.51      3198\n",
    "\n",
    "    accuracy                           0.61      7503\n",
    "   macro avg       0.59      0.59      0.59      7503\n",
    "weighted avg       0.60      0.61      0.60      7503\n",
    "\n",
    "Cross-validation MSE: 0.606 ± 0.078\n",
    "Training Set Accuracy: 0.898\n",
    "\n",
    "Evaluation Time Taken:  00:03:08\n",
    "Training Time Taken:   00:00:18\n",
    "```\n",
    "\n",
    "### 21 features 100 ensemble\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.66      0.79      0.72      4305\n",
    "         1.0       0.63      0.46      0.53      3198\n",
    "\n",
    "    accuracy                           0.65      7503\n",
    "   macro avg       0.65      0.63      0.63      7503\n",
    "weighted avg       0.65      0.65      0.64      7503\n",
    "\n",
    "Cross-validation MSE: 0.653 ± 0.083\n",
    "Training Set Accuracy: 0.798\n",
    "\n",
    "Evaluation Time Taken:  00:00:43\n",
    "Training Time Taken:   00:00:04\n",
    "```\n",
    "\n",
    "## Adaboost with entity features (100 ensemble)\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.66      0.79      0.72      4305\n",
    "         1.0       0.61      0.44      0.51      3198\n",
    "\n",
    "    accuracy                           0.64      7503\n",
    "   macro avg       0.63      0.62      0.61      7503\n",
    "weighted avg       0.64      0.64      0.63      7503\n",
    "\n",
    "Cross-validation MSE: 0.642 ± 0.083\n",
    "Training Set Accuracy: 0.801\n",
    "\n",
    "Evaluation Time Taken:  00:00:33\n",
    "Training Time Taken:   00:00:03\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with 21 features\n",
    "\n",
    "### Params: hidden_layer_sizes=(500, 500, 500), max_iter=2000\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.72      0.69      0.70      4305\n",
    "         1.0       0.60      0.64      0.62      3198\n",
    "\n",
    "    accuracy                           0.67      7503\n",
    "   macro avg       0.66      0.66      0.66      7503\n",
    "weighted avg       0.67      0.67      0.67      7503\n",
    "\n",
    "Cross-validation MSE: 0.665 ± 0.059\n",
    "Training Set Accuracy: 0.998\n",
    "\n",
    "Evaluation Time Taken:  09:23:30\n",
    "Training Time Taken:   00:19:10\n",
    "```\n",
    "\n",
    "### Params: `solver='lbfgs', alpha=1e-5, activation='tanh', hidden_layer_sizes=(100, 100, 100), max_iter=1000`\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.72      0.64      0.68      4305\n",
    "         1.0       0.58      0.66      0.62      3198\n",
    "\n",
    "    accuracy                           0.65      7503\n",
    "   macro avg       0.65      0.65      0.65      7503\n",
    "weighted avg       0.66      0.65      0.65      7503\n",
    "\n",
    "Cross-validation MSE: 0.649 ± 0.090\n",
    "Training Set Accuracy: 0.998\n",
    "\n",
    "Evaluation Time Taken:  00:47:30\n",
    "Training Time Taken:   00:05:01\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 hongyu hongyu 154M Mar 29 10:09 ../models/MLP-21-features.pkl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# pipeline.fit(X, y)\n",
    "\n",
    "mlp_mdl = '../models/MLP-21-features.pkl'\n",
    "joblib.dump(pipeline, mlp_mdl)\n",
    "!ls -lSh $mlp_mdl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
