{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    \n",
    "import numpy as np      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/train_set.csv\")\n",
    "test = pd.read_csv(\"./data/test_set.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  "
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format of the data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "110"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicates in the data (in regards to text)\n",
    "train.text.duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0    4342\n1    3271\nName: target, dtype: int64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check class imbalance\n",
    "train.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "id             0\nkeyword       61\nlocation    2533\ntext           0\ntarget         0\ndtype: int64"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look for NULL data\n",
    "train.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Clean-up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate values?\n",
    "# fill null rows?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model \n",
    "### CountVectorizer | RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector and an analyzer\n",
    "vectorizer = CountVectorizer()\n",
    "analyze = vectorizer.build_analyzer()\n",
    "\n",
    "dvec = vectorizer.fit_transform(train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive a test set from the train set\n",
    "dshuf = train.sample(frac=1)\n",
    "\n",
    "d_train = dshuf[:5000]\n",
    "d_test = dshuf[5000:]\n",
    "d_train_att = vectorizer.fit_transform(d_train['text'])  # fit bag-of-words on training set\n",
    "d_test_att = vectorizer.transform(d_test['text'])  # reuse on testing set\n",
    "d_train_label = d_train['target']\n",
    "d_test_label = d_test['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: At that stage, we have a training set that we want to perform a fit transform on, \n",
    "which means it will learn the words and also produce the matrix. However, for the \n",
    "testing set, we don't perform a fit transform again, since we don't want the model \n",
    "to learn from different words for the testing data. We will use the same words it \n",
    "learned on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.7822426329889016"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=80)\n",
    "clf.fit(d_train_att, d_train_label)\n",
    "clf.score(d_test_att, d_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1418,   82],\n       [ 487,  626]], dtype=int64)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a confusion matrix\n",
    "pred_labels = clf.predict(d_test_att)\n",
    "confusion_matrix(d_test_label, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Accuracy: 0.77 (+/- 0.02)'"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform cross-validation\n",
    "scores = cross_val_score(clf, d_train_att, d_train_label, cv=5)\n",
    "\n",
    "\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "dshuf = train.sample(frac=1)\n",
    "d_content = dshuf['text']\n",
    "d_label = dshuf['target']\n",
    "\n",
    "pipeline = make_pipeline(CountVectorizer(), RandomForestClassifier())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.7416980206118109"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the pipeline\n",
    "pipeline.fit(d_content[:1500], d_label[:1500])\n",
    "\n",
    "pipeline.score(d_content[1500:], d_label[1500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Accuracy: 0.77 (+/- 0.01)'"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform cross-validation\n",
    "scores = cross_val_score(pipeline, d_content, d_label, cv=5)\n",
    "\n",
    "\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Accuracy: 0.77 (+/- 0.02)'"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add TF-IDF (term frequencyâ€“inverse document frequency)\n",
    "pipeline2 = make_pipeline(CountVectorizer(),\n",
    "                          TfidfTransformer(norm=None),\n",
    "                          RandomForestClassifier())\n",
    "\n",
    "scores = cross_val_score(pipeline2, d_content, d_label, cv=5)\n",
    "\n",
    "\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   32.4s\n[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.9min\n[Parallel(n_jobs=-1)]: Done 216 out of 216 | elapsed:  3.2min finished\n"
    }
   ],
   "source": [
    "# Parameter search\n",
    "parameters = {\n",
    "    'countvectorizer__max_features': (None, 1000, 2000),\n",
    "    # unigrams or bigrams\n",
    "    'countvectorizer__ngram_range': ((1, 1), (1, 2)),\n",
    "    'countvectorizer__stop_words': ('english', None),\n",
    "    # effectively turn on/off tfidf\n",
    "    'tfidftransformer__use_idf': (True, False),\n",
    "    'randomforestclassifier__n_estimators': (20, 50, 100)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline2, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(d_content, d_label)\n",
    "\n",
    "\"Best score: %0.3f\" % grid_search.best_score_\n",
    "\"Best parameters set:\"\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    \"\\t%s: %r\" % (param_name, best_parameters[param_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Best score: 0.789\nBest parameters set:\n\tcountvectorizer__max_features: None\n\tcountvectorizer__ngram_range: (1, 1)\n\tcountvectorizer__stop_words: 'english'\n\trandomforestclassifier__n_estimators: 50\n\ttfidftransformer__use_idf: False\n"
    }
   ],
   "source": [
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}